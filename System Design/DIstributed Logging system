*** Distributed Logging System Design ***

Definition: A log file is a record that documents events within a software application. It captures details such as microservices, transactions, and service actions, providing a means to troubleshoot and monitor the application's flow.

Need for Logging:

1. Understanding Distributed Systems: Logging is crucial in comprehending the flow of events in a distributed system. It plays a vital role in pinpointing the timing and causes of system failures or security breaches, ultimately reducing downtime.

2. Efficient Debugging: Instead of relying on simple print statements, structured logging provides a more organized and efficient way to monitor applications. Print statements lack severity tracking and don't suit the needs of storing data in a structured manner.

3. Causality Management: Distributed systems often require causality information to ensure events are correctly sequenced across multiple nodes. Logging services effectively manage diagnostic and exploratory data in distributed software.

4. Performance and Error Detection: Logging aids in understanding code, locating unforeseen errors, and optimizing application performance. It provides insight into how processes are running in the system.

Log Analysis Benefits:
Troubleshooting: Logging is crucial for identifying and resolving application, node, or network issues.
Security and Compliance: It helps in adhering to internal security policies, external regulations, and compliance standards.
Data Breach Response: Log data is vital in recognizing and addressing data breaches and other security incidents.
User Behavior Insights: Log analysis is valuable for understanding user actions, which can inform recommender systems and user experience improvements.

Building a Distributed Log from Scratch, Part 1: Storage Mechanics
A log is an ordered, append-only data structure. It’s basically a sequence of unchangeable events. This simple concept has been used by developers for a long time, whether in application logs, system logs, or access logs. Logs typically consist of a timestamp and an event, appended to the end of a file. When we generalize this pattern, it becomes a powerful tool for managing and distributing data across systems.

Several systems like Apache Kafka, Amazon Kinesis, NATS Streaming, and Apache Pulsar use this idea. Kafka is especially well-known for popularizing it.

For a distributed log system to be effective, it needs to prioritize:
1. Performance: Fast enough to keep the data useful.
2. High Availability: Always accessible for data in and out.
3. Scalability: Able to grow with the enterprise's needs.

Applying the traditional pub/sub model to a log makes it a versatile solution for many problems.
Key Principles of Log Storage
  A log is an ordered, immutable sequence of messages. Each message is either fully in the log or not at all. Although we only add messages and never remove them, logs have retention policies to manage size. These policies could be based on time, number of messages, or size.
  The log can be read from any point, and it's stored on disk. Sequential disk access is relatively fast, especially with modern OS page caches that keep frequently accessed data in RAM. This makes reads and writes faster.

Basic Log Structure
  A log looks like a file where new messages are appended to the end. However, this file can become very large. To manage this, the log is split into segments. Each segment is a new file, and once a segment is full, a new one is created.
  Segments are identified by their base offset (the offset of the first message in the segment). This allows quick location of a message by doing a binary search.
  Each segment has an index file mapping message offsets to their positions in the segment. In Kafka, the index stores offsets and log positions efficiently.

Optimizing Data Access
  Ideally, data is written to the log in a format that can be sent directly over the network. This allows for zero-copy reads, where data moves from the disk to the network without entering user space, reducing CPU and memory usage.

Real-world Example
  In practice, systems like Kafka use the sendfile system call for zero-copy reads, which improves performance. NATS Streaming’s storage layer can use different backends, including file-based, in-memory, and SQL-backed storage.

Conclusion
  This overview of log storage mechanics sets the stage for understanding distributed log systems.

*** Building a Distributed Log from Scratch, Part 2: Data Replication ***

What a message log is, why it’s useful, and how its storage works.

We’ve got our log, and we know how to write and read data. But what if the machine storing the log dies? We’d be out of luck because our log would be gone. To avoid this single point of failure, we need high availability and fault tolerance. This means ensuring that the system keeps working even if a server fails, minimizing downtime without needing manual intervention.

Achieving High Availability
High availability means making sure we can always read and write data. If a server goes down, the system should still function. To do this, we eliminate the single point of failure by replicating the data. Replication also helps with scalability, but we’re focusing on high availability here.

There are two main ways to replicate log data: 
1. Gossip/Multicast Protocols: These include epidemic broadcast trees, bimodal multicast, SWIM, HyParView, and NeEM. They are generally eventually consistent and/or random.
2. Consensus Protocols: These include 2PC/3PC, Paxos, Raft, Zab, and chain replication. They focus more on consistency than availability.

Since our log requires ordering and consistency, we’ll go with consensus-based replication.

Consensus-Based Replication
Consensus-based replication has two main parts:
1. Leader Election: Choosing a leader to manage write operations.
2. Data Replication: Ensuring data is copied to the other servers (followers).

Leader Election
Leader election can be done through configuration, but for fault tolerance, the leader needs to be dynamic. If the leader crashes, we need a new one to take over. This is a well-studied problem and can be handled with algorithms like Raft.

Data Replication
Once we have a leader, it needs to replicate data to the followers. There are two approaches:
- All Replicas: Wait for all replicas to confirm the write.
  - Pros: Can handle multiple failures with fewer replicas.
  - Cons: Slower because it waits for the slowest replica.

- Quorum: Wait for a majority of replicas to confirm the write.
  - Pros: Faster because it doesn’t wait for the slowest replica.
  - Cons: Needs more replicas to handle the same number of failures.

Kafka uses the all replicas approach (with some conditions), while NATS Streaming uses a quorum approach. Let’s explore how each handles replication.

Kafka’s Replication
In Kafka, the leader manages an in-sync replica set (ISR) of all fully caught-up replicas. All reads and writes go through the leader. The leader writes messages to a write-ahead log (WAL). These messages are initially uncommitted or “dirty.” They are only committed once all replicas in the ISR have written them to their own WALs. The leader also keeps a high-water mark (HW) of the last committed message.

Producers can choose how they receive acknowledgments:
- Wait until the message is committed.
- Wait until the message is written to the leader’s WAL.
- Not wait at all.

Handling Failures in Kafka
1. Leader Fails: Kafka uses ZooKeeper to handle leader election. If the leader dies, ZooKeeper notifies the cluster controller, which then selects a new leader from the ISR.
2. Follower Fails: The leader tracks how caught up each follower is. If a follower falls behind, it’s removed from the ISR, making the cluster under-replicated until the follower catches up.
3. Follower Temporarily Partitioned: If a follower is temporarily unavailable, it’s treated like a failure. Once it catches up, it’s added back to the ISR.

NATS Streaming’s Replication
NATS Streaming uses the Raft consensus algorithm for leader election and data replication. This makes it different from Kafka, as Raft handles both tasks in a single protocol.

Raft in NATS Streaming
Raft elects a leader, and this leader sends heartbeats to followers. Writes go through the leader’s WAL, and followers append these writes to their own WALs. Once a quorum of followers acknowledges the write, it’s committed.
NATS Streaming also uses Raft groups for replication:
- Metadata Raft Group: Manages client state.
- Topic Raft Groups: Manage messages and subscriptions.
Raft allows NATS Streaming to operate without ZooKeeper or another coordination service.

Challenges with Raft
1. Scaling Raft: As the number of topics grows, so does the number of Raft groups, increasing network traffic. One solution is to hash topics to a fixed number of Raft groups. Another is to use a layer on top of Raft, like CockroachDB’s MultiRaft.
2. Dual Writes: Messages are written to both the Raft log and the NATS Streaming log. To avoid redundancy, we can think of the Raft log as our primary log and use the NATS Streaming log as an index.

Ensuring Performance and Durability
- Performance: Configuring publisher acknowledgments, avoiding explicit fsync on the broker, batching writes, and using zero-copy reads.
- Durability: Ensuring quorum for data replication. In Kafka, configuring `min.insync.replicas` and producer acks helps achieve this.

Conclusion
Covered how to replicate data to achieve high availability and fault tolerance. Explored how Kafka and NATS Streaming handle replication and the challenges they face. 

*** Building a Distributed Log from Scratch, Part 3: Scaling Message Delivery ***
Data Scalability
When you're dealing with a ton of data, you need to partition it to scale things up. Partitioning means you can spread the load across multiple nodes, making the system horizontally scalable. Kafka, for example, was built with this in mind. In Kafka, topics are split into partitions, and ordering is only guaranteed within each partition.

Let's say you have an e-commerce app with two topics: purchases and inventory, each with two partitions. This setup allows you to spread the reads and writes across multiple brokers. But here's the trick: how do you decide what data goes into which partition?

You could just distribute data randomly, but that messes up the order of events. For instance, if you have operations to add and remove inventory, you might end up processing a removal before an addition if they land in different partitions. To keep things ordered, you might hash a key and send all related writes to the same partition. For example, you could partition purchases by account name and inventory by SKU. That way, all actions for the same account or SKU stay in order.

Challenges with NATS Streaming
NATS Streaming doesn't handle partitioning as neatly as Kafka. Channels in NATS Streaming are fully ordered, like a single Kafka partition. So, if you want to partition data, you have to manage it at the application level. Some might find this simpler than Kafka's approach, but it wasn't designed with scalability as a primary goal.

Consumer Scalability
Scaling to a large number of consumers is another challenge. In Kafka and NATS Streaming, the leader handles all reads (and writes). Amazon Kinesis, for example, only supports up to five reads per second per shard. If you have more consumers than that, you're out of luck.
One idea is to partition your workload to increase parallelism or daisy chain streams to increase fan-out. But these solutions aren't great if you need to support thousands of IoT devices, for instance. With an immutable log, though, you don't have to worry about stale reads. The data is always the same, whether you read from the leader or a follower that's catching up.
In NATS Streaming, you could use non-voter replicas to serve reads. These replicas don't participate in leader elections or quorums; they just receive committed log entries. This setup effectively increases your read capacity without adding much complexity.

Push vs. Pull
Kafka uses a pull-based model for consumers, while NATS Streaming uses push. In a push model, the broker sends data to consumers, which requires explicit flow control to avoid overwhelming them. With pull, consumers request data at their own pace, simplifying flow control.
Push models can struggle with balancing latency and throughput. You might use a backoff protocol to optimize this, but it's tricky. Pull models, on the other hand, naturally lend themselves to batching. Consumers fetch all available messages, removing the guesswork around tuning.
From an API perspective, Kafka clients are complex because they do a lot of the heavy lifting, while NATS Streaming clients are simpler since the server handles more. This design decision shifts complexity based on the approach, but a smart client and dumb server might be more scalable in the long run.

Bookkeeping
Tracking the position in the log can be done by the server or the consumer. NATS Streaming keeps track of positions for consumers, which makes it easy for them to pick up where they left off. But in a clustered setup, this means more data replication, affecting performance.
An alternative is for consumers to track their positions, but they might not have fast storage, especially in IoT devices or containers. A middle ground is storing offsets in the log itself, like Kafka does. Clients periodically checkpoint their offsets, and log compaction keeps only the latest offsets. This way, the log itself becomes the fault-tolerant and durable storage for offsets.

Conclusion
Scale message delivery by partitioning data and managing consumers and the differences between push and pull models and how to handle bookkeeping.


Building a Distributed Log from Scratch, Part 4: Trade-Offs and Lessons Learned
Let’s look at some key trade-offs in building these systems and share lessons learned from creating NATS Streaming.
Competing Goals
When building a distributed log, you juggle several goals: performance, high availability, and scalability. Achieving all three can be tricky because they often conflict.
  - Performance vs. Fault Tolerance: A single-server system can be very fast because it only has to handle network speed and disk I/O. However, if the server fails, you risk losing data unless you have a standby server with replicated data.
  - High Availability: This means not only keeping the service running but also ensuring that any acknowledged data is not lost. With a standby server, you need data replication to prevent data loss.
  - Scalability: A single node has limited capacity. Eventually, you'll need to partition the data across multiple nodes to handle more load. This can complicate the system, especially if partitioning is handled at the application level.
NATS Streaming started as a single-node system, which was a concern for production due to its single point of failure. To address this, a fault-tolerance mode was added where a group of servers would run, with only one acting as the active server. However, this required a shared storage layer and did not replicate data unless the storage layer handled it, impacting performance.
Simplicity vs. Performance and Scalability
Simple designs often perform better but may not scale well or be fault-tolerant. Here are some examples from NATS Streaming:
  - Single-Node Design: Simple and fast but limited in capacity and not fault-tolerant.
  - Simple File Structure for Logs: Allows efficient use of hardware and OS capabilities, like sequential disk access.
  - Pull-Based Model: Reduces complexity around flow control and batching.
However, optimizing for user experience (UX) can make performance harder. NATS Streaming initially prioritized UX, leading to a simpler, more ergonomic API. But this shifted complexity to the server, making it harder to scale and replicate state across a cluster.
Key Lessons
  - Keep it Simple: Distributed systems are complex enough. Start with a simple system that works and then optimize it.
  - Leverage Existing Solutions: Use proven protocols and implementations like Raft for leader election and data replication. It’s easier to make a correct solution fast than to make a fast solution correct.
  - Test Thoroughly: Write extensive tests, including edge cases, and use formal methods and property-based testing. Chaos and fault-injection testing can also help.
  - Be Honest with Users: Clearly communicate design decisions, trade-offs, and system guarantees. Transparency builds trust and sets realistic expectations.
In conclusion, building a distributed log involves balancing performance, high availability, scalability, and simplicity. By learning from existing solutions and maintaining transparency, you can create a robust and reliable system.

Building a Distributed Log from Scratch, Part 5: Sketching a New System
In the fourth part of this series, key trade-offs involved in implementing a distributed log were examined, along with lessons learned while building NATS Streaming. In this fifth and final installment, the series concludes by outlining the design for a new log-based system that builds upon the concepts discussed in the previous entries.
The Context
To provide some context, NATS and NATS Streaming are distinct entities. NATS Streaming is a log-based streaming system built on top of NATS, which is a lightweight pub/sub messaging system. Originally, NATS was developed and open-sourced as the control plane for Cloud Foundry. In response to community requests for higher-level guarantees—such as durability and at-least-once delivery—NATS Streaming was created as a separate layer on top of NATS. NATS can be likened to a dial tone—ubiquitous and always on, ideal for “online” communications. In contrast, NATS Streaming serves as the voicemail system—where messages are stored and addressed later. While there are more nuances, this analogy captures the essence of their relationship.
A crucial point is that NATS and NATS Streaming are separate systems with distinct protocols, APIs, and client libraries. NATS Streaming was designed to function essentially as a client to NATS, meaning clients don’t interact directly with NATS Streaming; instead, all communication passes through NATS. However, the NATS Streaming binary can be configured to either embed NATS or connect to a standalone deployment.
Architecturally, this setup supports the end-to-end principle by layering additional functionality rather than integrating it directly into the underlying infrastructure. This approach allows stronger guarantees to be built on top while avoiding the risk of being unable to remove them later. However, this architecture introduces several challenges.
Firstly, there is no "cross-talk" between NATS and NATS Streaming, meaning messages published to NATS are not visible in NATS Streaming, and vice versa. These systems share the same infrastructure but operate independently, which means message durability isn’t layered onto NATS but rather provided by a separate system with distinct semantics.
Secondly, since NATS Streaming operates as a "sidecar" to NATS and all communication flows through NATS, a bottleneck exists at the NATS connection. Although this may only be a theoretical limit, it prevents certain optimizations, such as zero-copy reads of the log, and introduces reliance on timeouts even in cases where immediate server responses would be possible.
Thirdly, NATS Streaming lacks a robust mechanism for linear scaling, aside from running multiple clusters and partitioning channels at the application level. As clustering matures, it’s hoped this aspect will improve.
Fourthly, NATS Streaming’s authorization is inherently limited to what NATS provides, as all communication passes through it. While NATS supports multi-user authentication and subject-level permissions, the opaque protocol used by NATS Streaming complicates setting up proper ACLs at the streaming level.
Fifthly, NATS Streaming does not support wildcard semantics, which is a significant feature in NATS that users expect. NATS supports two wildcards in subject subscriptions—asterisk (*) and full wildcard (>)—but these are not supported in NATS Streaming, which stems more from the log design than the overall architecture.
More generally, clustering and data replication were added to NATS Streaming as afterthoughts, leading to increased server complexity, especially given the APIs that NATS Streaming provides, which handle flow control and track consumer state.