Distributed Logging System Design

Definition: A log file is a record that documents events within a software application. It captures details such as microservices, transactions, and service actions, providing a means to troubleshoot and monitor the application's flow.

Need for Logging:

1. Understanding Distributed Systems: Logging is crucial in comprehending the flow of events in a distributed system. It plays a vital role in pinpointing the timing and causes of system failures or security breaches, ultimately reducing downtime.

2. Efficient Debugging: Instead of relying on simple print statements, structured logging provides a more organized and efficient way to monitor applications. Print statements lack severity tracking and don't suit the needs of storing data in a structured manner.

3. Causality Management: Distributed systems often require causality information to ensure events are correctly sequenced across multiple nodes. Logging services effectively manage diagnostic and exploratory data in distributed software.

4. Performance and Error Detection: Logging aids in understanding code, locating unforeseen errors, and optimizing application performance. It provides insight into how processes are running in the system.

Log Analysis Benefits:
Troubleshooting: Logging is crucial for identifying and resolving application, node, or network issues.
Security and Compliance: It helps in adhering to internal security policies, external regulations, and compliance standards.
Data Breach Response: Log data is vital in recognizing and addressing data breaches and other security incidents.
User Behavior Insights: Log analysis is valuable for understanding user actions, which can inform recommender systems and user experience improvements.

Building a Distributed Log from Scratch, Part 1: Storage Mechanics
A log is an ordered, append-only data structure. It’s basically a sequence of unchangeable events. This simple concept has been used by developers for a long time, whether in application logs, system logs, or access logs. Logs typically consist of a timestamp and an event, appended to the end of a file. When we generalize this pattern, it becomes a powerful tool for managing and distributing data across systems.

Several systems like Apache Kafka, Amazon Kinesis, NATS Streaming, and Apache Pulsar use this idea. Kafka is especially well-known for popularizing it.

For a distributed log system to be effective, it needs to prioritize:
1. Performance: Fast enough to keep the data useful.
2. High Availability: Always accessible for data in and out.
3. Scalability: Able to grow with the enterprise's needs.

Applying the traditional pub/sub model to a log makes it a versatile solution for many problems.
Key Principles of Log Storage
  A log is an ordered, immutable sequence of messages. Each message is either fully in the log or not at all. Although we only add messages and never remove them, logs have retention policies to manage size. These policies could be based on time, number of messages, or size.
  The log can be read from any point, and it's stored on disk. Sequential disk access is relatively fast, especially with modern OS page caches that keep frequently accessed data in RAM. This makes reads and writes faster.

Basic Log Structure
  A log looks like a file where new messages are appended to the end. However, this file can become very large. To manage this, the log is split into segments. Each segment is a new file, and once a segment is full, a new one is created.
  Segments are identified by their base offset (the offset of the first message in the segment). This allows quick location of a message by doing a binary search.
  Each segment has an index file mapping message offsets to their positions in the segment. In Kafka, the index stores offsets and log positions efficiently.

Optimizing Data Access
  Ideally, data is written to the log in a format that can be sent directly over the network. This allows for zero-copy reads, where data moves from the disk to the network without entering user space, reducing CPU and memory usage.

Real-world Example
  In practice, systems like Kafka use the sendfile system call for zero-copy reads, which improves performance. NATS Streaming’s storage layer can use different backends, including file-based, in-memory, and SQL-backed storage.

Conclusion
  This overview of log storage mechanics sets the stage for understanding distributed log systems. In the next part, we’ll explore making the log fault-tolerant with data replication techniques.


Building a Distributed Log from Scratch, Part 2: Data Replication

What a message log is, why it’s useful, and how its storage works.

We’ve got our log, and we know how to write and read data. But what if the machine storing the log dies? We’d be out of luck because our log would be gone. To avoid this single point of failure, we need high availability and fault tolerance. This means ensuring that the system keeps working even if a server fails, minimizing downtime without needing manual intervention.

Achieving High Availability
High availability means making sure we can always read and write data. If a server goes down, the system should still function. To do this, we eliminate the single point of failure by replicating the data. Replication also helps with scalability, but we’re focusing on high availability here.

There are two main ways to replicate log data: 

1. Gossip/Multicast Protocols: These include epidemic broadcast trees, bimodal multicast, SWIM, HyParView, and NeEM. They are generally eventually consistent and/or random.
   
2. Consensus Protocols: These include 2PC/3PC, Paxos, Raft, Zab, and chain replication. They focus more on consistency than availability.

Since our log requires ordering and consistency, we’ll go with consensus-based replication.

Consensus-Based Replication

Consensus-based replication has two main parts:

1. Leader Election: Choosing a leader to manage write operations.
2. Data Replication: Ensuring data is copied to the other servers (followers).

Leader Election

Leader election can be done through configuration, but for fault tolerance, the leader needs to be dynamic. If the leader crashes, we need a new one to take over. This is a well-studied problem and can be handled with algorithms like Raft.

Data Replication

Once we have a leader, it needs to replicate data to the followers. There are two approaches:

- All Replicas: Wait for all replicas to confirm the write.
  - Pros: Can handle multiple failures with fewer replicas.
  - Cons: Slower because it waits for the slowest replica.

- Quorum: Wait for a majority of replicas to confirm the write.
  - Pros: Faster because it doesn’t wait for the slowest replica.
  - Cons: Needs more replicas to handle the same number of failures.

Kafka uses the all replicas approach (with some conditions), while NATS Streaming uses a quorum approach. Let’s explore how each handles replication.

Kafka’s Replication

In Kafka, the leader manages an in-sync replica set (ISR) of all fully caught-up replicas. All reads and writes go through the leader. The leader writes messages to a write-ahead log (WAL). These messages are initially uncommitted or “dirty.” They are only committed once all replicas in the ISR have written them to their own WALs. The leader also keeps a high-water mark (HW) of the last committed message.

Producers can choose how they receive acknowledgments:
- Wait until the message is committed.
- Wait until the message is written to the leader’s WAL.
- Not wait at all.

Handling Failures in Kafka

1. Leader Fails: Kafka uses ZooKeeper to handle leader election. If the leader dies, ZooKeeper notifies the cluster controller, which then selects a new leader from the ISR.
   
2. Follower Fails: The leader tracks how caught up each follower is. If a follower falls behind, it’s removed from the ISR, making the cluster under-replicated until the follower catches up.

3. Follower Temporarily Partitioned: If a follower is temporarily unavailable, it’s treated like a failure. Once it catches up, it’s added back to the ISR.

NATS Streaming’s Replication

NATS Streaming uses the Raft consensus algorithm for leader election and data replication. This makes it different from Kafka, as Raft handles both tasks in a single protocol.

Raft in NATS Streaming

Raft elects a leader, and this leader sends heartbeats to followers. Writes go through the leader’s WAL, and followers append these writes to their own WALs. Once a quorum of followers acknowledges the write, it’s committed.

NATS Streaming also uses Raft groups for replication:
- Metadata Raft Group: Manages client state.
- Topic Raft Groups: Manage messages and subscriptions.

Raft allows NATS Streaming to operate without ZooKeeper or another coordination service.

Challenges with Raft

1. Scaling Raft: As the number of topics grows, so does the number of Raft groups, increasing network traffic. One solution is to hash topics to a fixed number of Raft groups. Another is to use a layer on top of Raft, like CockroachDB’s MultiRaft.
