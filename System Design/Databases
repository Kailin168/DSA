Introduction to Databases:

  Databases are organized collections of data that make it easier to store, retrieve, modify, and delete data. They are essential for managing large amounts of data and have various advantages over file storage. Unlike files, databases offer concurrent management, grant different access rights to users, and allow for scalability and efficient search capabilities.

  There are two basic types of databases: SQL (relational databases) and NoSQL (non-relational databases). SQL databases have predetermined schemas and are suitable for structured data like contact information, while NoSQL databases are unstructured, dynamic, and suitable for storing diverse data types like user preferences.

  Advantages of using databases include managing large data, ensuring data consistency and integrity, enabling easy data updates, providing security and availability, and offering scalability through replication and partitioning.

  To delve deeper into databases, the chapter is divided into four lessons: Types of Databases, Data Replication, Data Partitioning, and Cost-benefit analysis. These lessons cover different database types, replication models, partitioning models, and the best sharding approach for different databases. Let's start by understanding the different types of databases and their preferred use cases.

SQL vs NoSQL

  SQL is relational 
  > For structured data
  > Table-based
  > Vertical scalable
  > Uses Foreign key, Primary key, Unique key
  > Use case: banking and e-commerce 
  > SQLite and PostgreSQL


  NoSQL is non relational
  > For semi-structured, structured, and un-structured data
  > NoSQL databases are document, key-value, graph, or wide-column stores
  > NoSQL is better for unstructured data like documents or JSON
  > Horizontal scalable
  > MongoDB



Relational databases store data in structured tables and use SQL for data manipulation. They provide simplicity, robustness, flexibility, and compatibility. They ensure data integrity through ACID properties (atomicity, consistency, isolation, durability). Popular relational database management systems (DBMS) include MySQL, Oracle Database, SQL Server, DB2, Postgres, and SQLite.

Non-relational (NoSQL) databases are designed for managing unstructured and semi-structured data with high scalability and flexibility. They offer simpler design, horizontal scaling, availability, and support for diverse data models. NoSQL databases include key-value databases (e.g., DynamoDB, Redis), document databases (e.g., MongoDB, Firestore), graph databases (e.g., Neo4J, OrientDB), and columnar databases (e.g., Cassandra, HBase).

Choosing the right database depends on factors such as data structure, ACID requirements, data size, and scalability needs. The distinctions between NoSQL and relational databases have blurred over time, with some NoSQL databases supporting SQL-like queries and ACID properties.

Data is a valuable asset for organizations as it provides crucial insights and drives the entire business. Organizations must securely store and retrieve client data while ensuring timely access under varying conditions. To successfully run an online business, the data store needs to possess characteristics such as fault tolerance, scalability, and high performance. Achieving these characteristics on a single node can be challenging or even impossible.

Replication 
  Replication is the process of maintaining multiple copies of data across various nodes to achieve availability, scalability, and performance. Replication is necessary because achieving these characteristics on a single node is challenging or impossible. However, replication comes with its complexities.

  Some complexities in replication include ensuring consistency among the copies, handling failed replica nodes, choosing between synchronous and asynchronous replication, managing replication lag in asynchronous replication, handling concurrent writes, and determining the appropriate consistency model for end programmers.

Synchronous versus asynchronous replication
  Synchronous replication waits for acknowledgments from secondary nodes before reporting success to the client, ensuring all nodes are up to date. However, if a secondary node fails to acknowledge, it causes high latency.

  Asynchronous replication doesn't wait for acknowledgments, allowing the primary node to continue its work even if secondary nodes are down. However, if the primary node fails, unwritten data can be lost.

  This presents a trade-off between data consistency and availability when system components can fail.

The primary-secondary replication model involves replicating data across multiple nodes, with one designated as the primary. The primary node processes writes and keeps the secondary nodes in sync. This model works well for read-heavy workloads and provides read resilience in case of primary node failure. However, it can become a bottleneck for writes and may result in data inconsistency if asynchronous replication is used. Missed updates can be lost if the primary node fails.

Data Partitioning 
  Data is partitioned or sharded to address the scalability challenges posed by increasing data volume and concurrent read/write traffic. Traditional databases may struggle to handle the load on a single node, resulting in reduced latency and throughput. By partitioning data, it can be distributed across multiple nodes, allowing for improved scalability.

  Partitioning data enables organizations to maintain the desirable properties of traditional databases, such as range queries, secondary indices, and ACID transactions, while benefiting from the distributed nature of the database system. It offers the opportunity to optimize performance for specific use cases and achieve better results compared to generic solutions.

  Data partitioning involves dividing the data into subsets and assigning each subset to a different node. The goal is to achieve balanced partitions and distribute the read/write load evenly across the nodes. This approach helps in managing increasing query rates and larger data sizes more effectively.

Sharding
Sharding is a technique used to distribute the load among multiple nodes in a network by partitioning or dividing a large dataset into smaller chunks of data stored on different nodes. The goal is to balance the partitions so that each partition contains a similar amount of data.

There are two common ways to shard or partition data:

1. Vertical Sharding: In vertical sharding, different tables or columns within a table are placed in separate database instances or servers. For example, a table may be split into multiple tables, with some columns in one table and the remaining columns in another. This approach is often used to improve the retrieval speed of tables containing wide text or binary large objects (BLOBs). By splitting such columns into separate tables, data retrieval and manipulation become more efficient.

2. Horizontal Sharding: Horizontal sharding involves dividing a table into multiple tables based on rows of data. Each partition or shard of the original table is distributed across different database servers. There are two common strategies for horizontal sharding:

   - Key-range based sharding: In this approach, each partition is assigned a continuous range of keys. For example, a partition may be assigned a range of customer IDs. This allows range queries to be performed efficiently using the partitioning keys. Tables with foreign key relationships can also be horizontally partitioned using the same partition key on all related tables.
   
   - Hash-based sharding: Hash-based sharding involves applying a hash-like function to a specific attribute or key, which produces different values based on the attribute's value. The hash value is then used to determine the partition in which the data should be stored. This technique ensures that keys are uniformly distributed across the partitions. However, it doesn't support range queries since keys are spread across all partitions.
