System Design Uber

Uber is a ride-hailing service that connects riders with drivers using a mobile app. Here's a simplified overview of its system design:

What is Uber?

Uber is an app-based platform for ride-hailing services.
It allows users to book rides from one location to another using the app.
Drivers register their vehicles and offer rides to users through the app.

Uber, despite its seemingly simple user experience, relies on a complex infrastructure to make each ride happen. Behind the scenes, there's a vast network of services and a huge volume of data to support the platform.

Uber's backend started as a single, monolithic software architecture using Python and SQLAlchemy for database management. However, it has since evolved into a service-oriented architecture with hundreds of services.

Uber's backend doesn't just handle ride hailing but also food delivery and cargo services. It primarily serves mobile app traffic over mobile data.

At its core, Uber uses a Dispatch system that acts like a real-time market, matching drivers (supply) with riders (demand).

- Supply Service tracks cars using geolocation and various attributes like the number of seats and the presence of car seats or wheelchair accessibility. It also manages vehicle allocation.

- Demand Service tracks rider GPS locations and their requirements for rides.

- DISCO (Dispatch Optimization) is the service that matches riders to drivers in real-time, ensuring efficient routing and minimizing waiting times.

Uber's geospatial design divides the Earth into small cells using Google S2 library. These cells have unique IDs used as sharding keys for scaling.

The system uses Apache Kafka for data transfer. GPS locations from cabs are sent to Kafka, loaded into memory, and then persisted to a database. Mapping and routing utilize historical travel times and AI algorithms.

Scaling Dispatch is achieved through Node.js and Ringpop, a protocol for distributed applications. It uses gossip protocols for cooperation and coordination.

The supply and demand data is stored in various databases, including Redis and custom clustering. MySQL is used to handle trip data, while Apache Kafka is used for location data.

Analyzing log data involves pushing logs to a Kafka cluster, applying filters with log stash, and analyzing with tools like Elasticsearch or Graphana.

Uber has systems in place for fraud detection, payment processing, promotions, and coupons. Load balancing is achieved at multiple layers (Layer 3, Layer 4, and Layer 7).

After a trip, post-trip actions include collecting ratings, sending emails, updating databases, and scheduling payments.

Uber's surge pricing model increases fares during high demand, which is determined by prediction algorithms.

In case of a total data center failure, Uber maintains a backup data center and relies on driver phones as a source of trip data. It sends encrypted state digests to driver phones, which can be used to recover trip information when needed.

So, while the Uber app may seem straightforward to users, it relies on a sophisticated and robust backend infrastructure to manage the complexities of ride hailing and related services.

Requirements:
  For Customers:
  - See nearby cabs with estimated time of arrival (ETA) and pricing.
  - Book a cab to a destination.
  - Track the location of the driver.
  For Drivers:
  - Accept or deny ride requests.
  - See the pickup location of the customer.
  - Mark trips as complete upon reaching the destination.

Non-Functional Requirements
  We need high reliability, availability with low latency, scalability, and efficiency.

Extended Features
  Customers can rate trips, process payments, and collect metrics and analytics.

Estimation
  Assuming 100 million daily active users (DAU), 1 million drivers, and 10 million rides daily, we'd handle about 1 billion requests daily, translating to about 12K requests per second.

Storage and Bandwidth
  With an average message size of 400 bytes, we'd need 400 GB of storage daily and about 1.4 PB over 10 years. Bandwidth requirements would be around 5 MB/s.

Data Model
  We'd use tables like 'customers', 'drivers', 'trips', 'cabs', 'ratings', and 'payments' in our database.

Database Choice
  We could use PostgreSQL or Apache Cassandra, distributing data across services for scalability.