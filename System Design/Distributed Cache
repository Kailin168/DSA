The Distributed Cache
  Problem statement:
  - A system consists of clients, service hosts, and a database.
  - As the number of users increases, database queries overwhelm the service providers, leading to slow performance.
  - To address this, a cache is added to improve performance by storing frequently accessed data in memory.

  What is a distributed cache?
  - A distributed cache is a system where multiple cache servers work together to store frequently accessed data.
  - Distributed caches are used when a single cache server is insufficient to store all the data and scalability and availability are important.

  Benefits of distributed cache:
  - Minimizes user-perceived latency by storing frequently accessed data and precalculating results.
  - Optimizes expensive database queries by pre-generating them in the cache.
  - Temporarily stores user session data.
  - Continues to serve data from cache even if the data store is temporarily unavailable.
  - Reduces network costs by serving data from local resources.

  Reasons for using distributed cache:
  - Storing the entire data in one system becomes impractical due to size and potential single point of failure.
  - Caching at different layers of the system improves performance and decouples sensitive data.
  - Caching is performed at web, application, database, DNS, and client-side layers.

  Designing a distributed cache:
  - The design process involves building background knowledge, creating a high-level design, refining it for scalability and performance, evaluating non-functional requirements, and comparing industrial solutions like Memcached and Redis.

  Understanding the basics of a distributed cache helps address performance issues in systems with a large number of users by efficiently storing and serving frequently accessed data. By distributing the cache across multiple servers, scalability and availability can be ensured, resulting in improved system performance and reduced latency.


Background of Distributed Cache
    Learn the fundamentals for designing a distributed cache. Here are the key points covered:
    Writing policies:
    - Discusses different strategies for writing data to cache and databases, such as write-through, write-back, and write-around.
    - Considers the implications of each strategy on consistency models.

    Eviction policies:
    - Explains the need for eviction mechanisms in small caches with limited storage capacity.
    - Mentions well-known eviction strategies like LRU, MRU, LFU, and MFU.
    - Emphasizes that the choice of eviction algorithm depends on the system requirements.

    Cache invalidation:
    - Addresses the issue of stale or outdated data in the cache.
    - Introduces time-to-live (TTL) value as a way to identify and remove expired entries.
    - Describes active and passive expiration methods for dealing with outdated items.

    Storage mechanism:
    - Explores design considerations for storing data in a distributed cache with multiple cache servers.
    - Discusses the use of hash functions to determine cache server selection and locate cache entries.
    - Mentions the use of doubly linked lists for data structure simplicity and efficiency.

    Sharding in cache clusters:
    - Introduces sharding as a technique to distribute cache data among multiple cache servers.
    - Presents two approaches: dedicated cache servers and co-located cache.
    - Highlights the advantages and considerations of each approach.

    Cache client:
    - Explains the role of a cache client in performing hash calculations, storing, and retrieving data from cache servers.
    - Mentions coordination with other system components and the use of standard transport protocols.

    Conclusion:
    - Summarizes the importance of distributed caches in distributed systems.
    - Reiterates the significance of understanding the design principles and mechanisms involved in building a distributed cache.

    Understanding the fundamentals of distributed cache design is essential for efficiently managing data storage, eviction, and cache invalidation. Sharding techniques and different cache client strategies contribute to the overall performance and scalability of a distributed cache system.

High-level design of a distributed cache, let's focus on the key points
    Requirements:
    - Functional: 
      - Insert data: Users should be able to insert entries into the cache.
      - Retrieve data: Users should be able to retrieve data based on a specific key.

    - Non-functional:
      - High performance: Both insertion and retrieval operations should be fast.
      - Scalability: The cache system should scale horizontally to handle increasing requests.
      - High availability: The system should be able to withstand failures of components, network, and power outages.
      - Consistency: Data stored in the cache servers should be consistent across different cache clients.
      - Affordability: The cache system should be designed using commodity hardware.

    API Design:
    - Insertion: The API call for insertion should be `insert(key, value)`, where the key is a unique identifier and the value is the data stored against the key. It should return an acknowledgment or an error.
    - Retrieval: The API call for retrieval should be `retrieve(key)`, where the key is used to retrieve the corresponding data. It should return the data object to the caller.

    Design Considerations:
    - Storage hardware: Consider whether specialized or commodity hardware should be used for cache partitions. The number of shard servers will depend on the cache size and access frequency. Secondary storage can be used for persistence, especially during cache rebuilding after a reboot.
    - Data structures: Hash tables are suitable for fast data access, and linked lists can be used for eviction algorithms.
    - Cache client: Decide the location of the client process, either within a serving host for internal use or as a dedicated cache client for external use.
    - Writing policy: Choose a writing strategy that suits the application's consistency requirements.
    - Eviction policy: Determine how to make room for new entries when the cache is full, considering eviction policies such as LRU (least recently used).
    - Time-to-live (TTL): Optimize the TTL value to reduce cache misses.

    High-Level Design:
    - Cache client: Resides in service application servers and holds information about cache servers. Uses a consistent hashing algorithm to select a cache server for each request.
    - Cache servers: Maintain the cache data and are accessible by all cache clients. Connected to the database for storing or retrieving data. Communication with cache clients is done using TCP or UDP protocols.

Detailed design of a distributed cache
    Find and Remove Limitations:
    - Lack of cache server awareness: Implement a configuration mechanism, such as a configuration file or a centralized configuration service, to provide cache client with updated information about cache servers.
    - Single point of failure: Add replica nodes to improve availability and handle failures. Use primary and backup nodes within a cache shard to ensure availability and consistency.
    - Internals of cache server: Each cache server should use a hash map to store and locate entries, a doubly linked list for eviction, and an eviction policy, such as least recently used (LRU).

    Improve Availability:
    - Use replica nodes to ensure availability in case of failures.
    - Divide cache data among shards to distribute load and avoid wasting hardware resources.
    - Primary-secondary node configuration allows for reads from multiple nodes in hot shards, improving performance.

    Internals of Cache Server:
    - Utilize a hash map to store and locate entries within the cache servers' RAM.
    - Employ a doubly linked list to order entries based on their frequency of access for eviction purposes.
    - Choose an eviction policy, such as LRU, depending on the application requirements.

    Detailed Design:
    - Client requests pass through load balancers to reach the service hosts where cache clients reside.
    - Consistent hashing is used by cache clients to identify the appropriate cache server and forward the request.
    - Each cache server includes primary and replica servers, all using the same internal mechanisms for storing and evicting cache entries.
    - Configuration service ensures cache clients have an updated and consistent view of the cache servers.
    - Monitoring services can be added to log and report caching service metrics.

    Note: The cache entries are stored and retrieved from RAM for fast access.

Evaluation of a distributed cache's design
  High Performance:
    - Consistent hashing allows efficient key retrieval with a time complexity of O(log(N)).
    - Hash tables and doubly linked lists are used internally in cache servers for fast access and eviction.
    - Communication between cache clients and servers is done through fast TCP and UDP protocols.
    - Adding more replicas improves performance under high request loads.
    - Storing and retrieving data from RAM ensures low latency.
  Scalability:
    - Shards can be created based on requirements and changing server loads.
    - Consistent hashing minimizes rehash computations when adding new cache servers.
    - Adding replicas reduces load on hot shards.
    - Further sharding or intelligent client-side solutions can be employed to handle hotkeys.
  High Availability:
    - Redundant cache servers improve availability and fault tolerance.
    - Leader-follower algorithm manages cluster shards.
    - Availability can be increased by distributing leader and follower servers across different data centers, but it may impact consistency.
    - Replication within a data center can provide strong consistency with good performance.
  Consistency:
    - Writing data to cache servers can be done synchronously or asynchronously.
    - Asynchronous mode is favored for caching systems to improve performance, but it may result in inconsistencies.
    - Strong consistency can be achieved with synchronous writing, but it increases latency and impacts performance.
    - Inconsistencies can arise from faulty configurations and services.
  Affordability:
    - The proposed design is cost-effective as it can be implemented using commodity hardware.
  In summary, the designed distributed cache provides high performance through efficient algorithms, scalability through sharding and replicas, high availability with redundancy, and affordability using commodity hardware. Consistency can be compromised for improved performance, and careful configuration is required to avoid inconsistencies.

Memcached and Redis are both widely adopted distributed caching tools, but they have some differences in terms of design, features, and use cases. Here's a simplified comparison of Memcached and Redis: