The Distributed Cache

  Problem statement:
  - A system consists of clients, service hosts, and a database.
  - As the number of users increases, database queries overwhelm the service providers, leading to slow performance.
  - To address this, a cache is added to improve performance by storing frequently accessed data in memory.

  What is a distributed cache?
  - A distributed cache is a system where multiple cache servers work together to store frequently accessed data.
  - Distributed caches are used when a single cache server is insufficient to store all the data and scalability and availability are important.

  Benefits of distributed cache:
  - Minimizes user-perceived latency by storing frequently accessed data and precalculating results.
  - Optimizes expensive database queries by pre-generating them in the cache.
  - Temporarily stores user session data.
  - Continues to serve data from cache even if the data store is temporarily unavailable.
  - Reduces network costs by serving data from local resources.

  Reasons for using distributed cache:
  - Storing the entire data in one system becomes impractical due to size and potential single point of failure.
  - Caching at different layers of the system improves performance and decouples sensitive data.
  - Caching is performed at web, application, database, DNS, and client-side layers.

  Designing a distributed cache:
  - The design process involves building background knowledge, creating a high-level design, refining it for scalability and performance, evaluating non-functional requirements, and comparing industrial solutions like Memcached and Redis.

  Understanding the basics of a distributed cache helps address performance issues in systems with a large number of users by efficiently storing and serving frequently accessed data. By distributing the cache across multiple servers, scalability and availability can be ensured, resulting in improved system performance and reduced latency.


Background of Distributed Cache
    Learn the fundamentals for designing a distributed cache. Here are the key points covered:

    Writing policies:
    - Discusses different strategies for writing data to cache and databases, such as write-through, write-back, and write-around.
    - Considers the implications of each strategy on consistency models.

    Eviction policies:
    - Explains the need for eviction mechanisms in small caches with limited storage capacity.
    - Mentions well-known eviction strategies like LRU, MRU, LFU, and MFU.
    - Emphasizes that the choice of eviction algorithm depends on the system requirements.

    Cache invalidation:
    - Addresses the issue of stale or outdated data in the cache.
    - Introduces time-to-live (TTL) value as a way to identify and remove expired entries.
    - Describes active and passive expiration methods for dealing with outdated items.

    Storage mechanism:
    - Explores design considerations for storing data in a distributed cache with multiple cache servers.
    - Discusses the use of hash functions to determine cache server selection and locate cache entries.
    - Mentions the use of doubly linked lists for data structure simplicity and efficiency.

    Sharding in cache clusters:
    - Introduces sharding as a technique to distribute cache data among multiple cache servers.
    - Presents two approaches: dedicated cache servers and co-located cache.
    - Highlights the advantages and considerations of each approach.

    Cache client:
    - Explains the role of a cache client in performing hash calculations, storing, and retrieving data from cache servers.
    - Mentions coordination with other system components and the use of standard transport protocols.

    Conclusion:
    - Summarizes the importance of distributed caches in distributed systems.
    - Reiterates the significance of understanding the design principles and mechanisms involved in building a distributed cache.

    Understanding the fundamentals of distributed cache design is essential for efficiently managing data storage, eviction, and cache invalidation. Sharding techniques and different cache client strategies contribute to the overall performance and scalability of a distributed cache system.