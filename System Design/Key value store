Key-value store:

Introduction to key-value stores:
Key-value stores are like distributed hash tables (DHTs). Each key is generated using a hash function and should be unique. In a key-value store, a key is associated with a specific value, and the value can be anything the user wants to store, like a blob, image, or server name. Key-value stores are useful in various scenarios, such as storing user sessions in web applications or building NoSQL databases.

Designing a key-value store:
To design a key-value store, we'll go through the following four steps:

1. Design the Key-value Store:
   - Define the requirements of the key-value store and design the application programming interface (API) for accessing the data.
   Sure! Here's a simplified explanation of the design of a key-value store:

      Requirements:
      To design a key-value store, we need to consider both functional and non-functional requirements.

      Functional requirements:
      1. Configurable service: The key-value store should be configurable to support different consistency models, allowing applications to make trade-offs between availability, consistency, cost-effectiveness, and performance.

      2. Ability to always write: Applications should have the ability to write data into the key-value store. However, if strong consistency is required, this might not always be possible due to the implications of the CAP theorem.

      3. Hardware heterogeneity: Each node in the system should be capable of performing any task, ensuring that there are no distinguished nodes. While servers can have different capabilities, newer hardware should be able to handle tasks as well as older hardware.

      Non-functional requirements:
      1. Scalability: The key-value store should be able to run on tens of thousands of servers distributed globally, allowing for incremental scalability. Servers should be added or removed with minimal disruption, and the system should handle a large number of users.

      2. Availability: Continuous service availability is crucial. The system should provide the desired level of availability, which can be configured based on the consistency requirements. Strong consistency might result in lower availability and vice versa.

      3. Fault tolerance: The key-value store should continue operating even in the presence of failures in servers or their components, ensuring uninterrupted service.

      API design:
      The API design of a key-value store typically includes two primary functions: get and put.

      1. The get function:
        - API call: get(key)
        - Description: Returns the value associated with the provided key. If data is replicated and the store is configured with weaker data consistency, multiple values might be returned for the same key.

      2. The put function:
        - API call: put(key, value)
        - Description: Stores the provided value associated with the given key. The system determines where the data should be placed, and it may keep metadata about the stored object, such as the version.

      Data type:
      In a key-value store, the key is often a primary key, while the value can be any arbitrary binary data. Some key-value stores, like Dynamo, use MD5 hashes on the key to generate a unique identifier that determines which server node is responsible for that key.


2. Ensure Scalability and Replication:
   - Learn how to achieve scalability using consistent hashing. Consistent hashing allows distributing the data across multiple servers in a balanced way.
   - Replicate the partitioned data to ensure fault tolerance and high availability.

      Add scalability:
      To achieve scalability in a key-value store, we need to partition the data across multiple storage nodes. This allows us to distribute the load evenly and add or remove nodes as needed. One way to achieve this is by using the modulus operator to assign requests to nodes based on their IDs. However, this method can be inefficient when adding or removing nodes, as a large number of keys need to be moved.

      Consistent hashing:
      Consistent hashing is an effective technique for load management in key-value stores. It involves considering a conceptual ring of hashes, where each node's ID is mapped to a point on the ring. Requests are also mapped to a point on the ring using their hash values. By moving in a clockwise direction on the ring, each request is processed by the next node encountered. This approach minimizes the number of keys that need to be moved when nodes are added or removed.

      Use of virtual nodes:
      To achieve a more even distribution of the request load, virtual nodes can be used. Instead of using a single hash function, multiple hash functions are applied to each key. This results in each server having multiple positions on the ring, allowing for a more uniform distribution of requests. Additional virtual nodes can be added to nodes with higher hardware capacity, further improving load balancing.

      Advantages of virtual nodes:
      Using virtual nodes provides several advantages:
      - Workload is uniformly distributed when a node fails or undergoes maintenance, as the other nodes share its load.
      - Each node can decide how many virtual nodes it is responsible for, taking into account its hardware capacity.
      - The proposed design becomes more scalable and can handle increased request loads.

      Data replication:
      Data replication ensures durability and high availability in the key-value store. There are two common approaches:

      1. Primary-secondary approach: In this approach, one storage area is designated as the primary, while others are secondary. The secondary areas replicate data from the primary, allowing for read requests. However, write requests can only be served by the primary. This approach introduces a single point of failure if the primary goes down.

      2. Peer-to-peer approach: In this approach, all storage areas are primary and replicate data among themselves. Both read and write operations are allowed on all nodes. Typically, a smaller number of nodes (e.g., three or five) are chosen for replication to avoid inefficiency and excessive replication.

      By using a peer-to-peer relationship for replication, data items are replicated across multiple hosts. Each node has a coordinator responsible for handling read and write operations for specific keys. The coordinator replicates the keys to a specified number of successors on the ring. Preference lists are used to determine the replicas, skipping nodes that already have a replica to ensure they are placed on different physical nodes.


3. Versioning Data and Achieving Configurability:
   - Resolve conflicts that can arise when multiple entities make changes to the same data.
   - Make the system more configurable to adapt to different use cases and requirements.

      Data versioning:
      - When there are network issues or failures during updates, the version history of an object can become fragmented, leading to multiple copies of the same data.
      - To avoid losing updates, it's important to handle conflicts and reconcile divergent histories for consistency.
      - Causality between events can be maintained using timestamps or vector clocks, which are lists of (node, counter) pairs representing the version of an object.
      - Vector clocks allow us to determine causal relationships and handle conflicts effectively.

      Modifying the API design:
      - The API design needs to include context information about the node and its vector clock value for determining causal relationships.
      - The "get" API call returns the object's value along with its metadata, including the version.
      - The "put" API call stores a value associated with a key and requires the context metadata.
      - If conflicts arise from multiple versions, the client is responsible for resolving them.

      Vector clock usage example:
      - An example scenario illustrates how vector clocks track versions and handle conflicts.
      - Each write operation is associated with a vector clock indicating the node and its counter value.
      - In case of network partitions, different nodes handle requests, leading to divergent versions.
      - After network repair, conflicts arise, and the client receives the conflicting versions' context.
      - The client performs reconciliation, and the write is coordinated, resulting in a new version with an updated clock.

4. Enable Fault Tolerance and Failure Detection:
   - Implement mechanisms to make the key-value store fault tolerant, meaning it can continue functioning even if some servers fail.
   - Develop techniques for detecting failures in the system and responding accordingly.

By following these steps, we can design a key-value store that is scalable, fault-tolerant, and configurable to meet different needs. This type of system is commonly used by major companies like Amazon, Facebook, and Netflix for various purposes such as managing user preferences, shopping carts, and product catalogs. It offers a simpler and more cost-effective alternative to traditional relational databases for certain applications.