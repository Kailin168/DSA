A rate limiter is a mechanism that puts a cap on the number of requests a service can handle within a specific time frame. It throttles incoming requests that exceed the predefined limit. For instance, if a service's API is set to allow 500 requests per minute, any client making more than 500 requests within that minute would be blocked from making further requests.

The need for a rate limiter arises as a defensive layer for services to prevent excessive usage, both intentional and unintentional. It protects against abusive behaviors targeting the application layer, such as denial-of-service (DOS) attacks and brute-force password attempts.

The following scenarios illustrate the usefulness of rate limiters in making services more reliable:
1. Preventing resource starvation: Rate limiters protect against friendly-fire denial-of-service incidents caused by errors or misconfigurations leading to resource starvation.
2. Managing policies and quotas: Rate limiters ensure fair and reasonable use of shared resources among multiple users by applying limits on time duration or quantity allocated.
3. Controlling data flow: In systems handling large volumes of data, rate limiters distribute the workload evenly among different machines to avoid burdening a single machine.
4. Avoiding excess costs: Rate limiting helps control the cost of operations, preventing experiments from running out of control and incurring large bills. Some cloud service providers use rate limiting to offer freemium services with limited usage.

To design a rate limiter, we'll cover the following steps:
1. Requirements: Discuss functional and non-functional requirements, types of throttling, and efficient locations for placing the rate limiter.
2. High-level design: Provide an overview of the rate limiter's high-level design.
3. Detailed design: Explore the building blocks involved in the detailed design of a rate limiter.
4. Rate limiter algorithms: Explain various algorithms essential for the operations of a rate limiter.

Requirements of a Rate Limiter's Design:
A rate limiter is designed with specific functional and non-functional requirements to limit the number of requests a client can make to an API within a defined time window. The requirements include:
Functional Requirements:
  1. Limit Requests: The rate limiter restricts the number of requests a client can send within a specified time frame.
  2. Configurable Limit: The limit of requests per time window should be configurable, allowing flexibility to adjust the rate limit as needed.
  3. Notification on Threshold Crossed: The rate limiter should notify the client with an error or notification when the defined threshold is exceeded, whether on a single server or across multiple servers.

Non-functional Requirements:
  1. Availability: The rate limiter must be highly available as it serves as a protective layer for the system.
  2. Low Latency: The rate limiter should operate with minimal latency to avoid impacting user experience.
  3. Scalability: The design should be highly scalable to handle an increasing number of client requests over time.

Types of Throttling:
A rate limiter can perform three types of throttling:
  1. Hard Throttling: Requests that exceed the predefined limit are discarded, allowing no further processing.
  2. Soft Throttling: A certain percentage of requests exceeding the limit (e.g., 5%) may be allowed for a more lenient approach.
  3. Elastic or Dynamic Throttling: Requests can exceed the predefined limit if there are excess resources available, without a specific percentage defined for the upper limit.

Placement of Rate Limiter:
The rate limiter can be placed in three different ways:
  1. Client-side: Placing the rate limiter on the client side is easy but insecure, as it can be tampered with by malicious activity.
  2. Server-side: Placing the rate limiter on the server side is more secure, and each server has its rate limiter to handle incoming requests.
  3. As Middleware: The rate limiter can act as middleware, intercepting requests to API servers and performing rate limiting functions.

Two Models for Rate Limiter Implementation:
There are two models for implementing rate limiters with multiple nodes:
  1. Rate Limiter with Centralized Database: Rate limiters interact with a centralized database like Redis or Cassandra to store and retrieve counters. It ensures that clients cannot exceed predefined limits but may face increased latency and race conditions under high concurrency.
  2. Rate Limiter with Distributed Database: Each node in the rate limiter cluster tracks rate limits independently, but clients could momentarily exceed the limit while sending requests to different nodes. This approach requires sticky sessions in the load balancer for proper enforcement.

Building Blocks Used:
The design of the rate limiter involves the use of databases, caches, and queues to store rules, metadata of users, and hold incoming requests, respectively.

Design of a Rate Limiter

To design effective rate limiters that control and manage resource consumption within a system.
  High-level design: Understand the overall structure of a rate limiter.
  Detailed design: Delve into the components and processes of a rate limiter.
  Request processing: Learn how the rate limiter handles incoming requests.
  Race condition: Discover potential issues with concurrency and how to address them.
  Rate limiter placement: Understand where to position the rate limiter for optimal performance.
  Conclusion: Sum up the key concepts of rate limiter design.
  High-level design
  A rate limiter is a distinct service that interacts with a web server to determine whether incoming requests should proceed or not. It follows predefined rules to decide if the request exceeds the allowed rate. These rules are defined by the service owner and include parameters such as the number of requests allowed per time unit. For instance, a rule might permit five marketing messages per day.

Detailed design
The detailed design of a rate limiter involves several components:
  1. Rule database: Stores rules established by the service owner, including the maximum requests allowed per time unit.
  2. Rules retriever: Periodically checks for rule updates in the database and updates the rule cache.
  3. Throttle rules cache: Contains cached rules for faster decision-making, enhancing performance.
  4. Decision-maker: Determines if a request exceeds the rate limit based on cached rules and specified algorithms.
  5. Client identifier builder: Generates a unique ID for incoming requests to distinguish clients. The rate limiter evaluates each request against cached rules to decide whether to allow, queue, or reject it. Requests exceeding the limit may trigger error responses or queueing mechanisms.

Request processing
Incoming requests are processed in a sequence of steps:
  1. The client identifier builder generates a unique ID for each request.
  2. The decision-maker evaluates the request based on cached rules and rate-limiting algorithms.
  3. Depending on the algorithm (hard, soft, or elastic throttling), the request is allowed, queued, or rejected.
  Race condition
  A race condition can occur in high-concurrency scenarios when multiple requests modify the same counter simultaneously. This can lead to incorrect rate limiting. Strategies to address this include using locking mechanisms, dividing the quota among multiple places, or implementing sharded counters.

Rate limiter placement
Rate limiters can be positioned in various ways:
  1. Client-side: Not recommended due to security concerns and difficulty in configuration.
  2. Server-side: Secure placement, each server has its rate limiter, enhancing reliability.
  3. Middleware: Acts as an intermediary between clients and API servers, a common practice.
    A rate limiter should not hinder the critical path of clientsâ€™ requests. To minimize latency, work can be divided into offline (pre-checking) and online (updating) phases, improving performance for high request volumes.

Conclusion
A well-designed rate limiter contributes to system reliability, low latency, and scalability. By effectively controlling the flow of requests, rate limiters prevent resource exhaustion and maintain a responsive system.