Web Crawler Basics

A web crawler is like a smart robot that explores the internet to gather information. It starts by visiting specific web addresses and collects data from those pages. This collected data is then stored for later use.

Why do we use web crawlers?

1. Testing Web Pages: Web crawlers help check if web page links and structures are correct.
2. Monitoring Changes: They keep an eye on updates in web page content or structure.
3. Site Mirroring: Web crawlers can effectively copy popular websites.
4. Copyright Check: They fetch content and look for copyright issues.

1. Requirements Clarification: Our web crawler needs to:
   - Receive a list of starting web addresses (seed URLs) to begin the search.
   - Follow links on web pages to explore more of the internet gradually.
   - Respect politeness policies, meaning setting a delay before revisiting a website.
   - Distinguish between links to files and links to other pages.
   - Prioritize crawling based on factors like regularity and content type.
   - Avoid indexing the same page more than once.
   - Meet non-functional requirements: reliability, scalability, and availability.
2. Design Approach: Our design approach involves several steps:
   - Back-of-the-Envelope Estimation: Calculate system performance needs based on estimated data size, storage, and bandwidth.
   - System Interface Design: Define how the system will interact with users and how pages will be fetched from the internet.
 